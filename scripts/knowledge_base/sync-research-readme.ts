import { readFile, writeFile } from "node:fs/promises";
import { dirname, resolve } from "node:path";
import { fileURLToPath } from "node:url";

type ResearchEntry = {
  slug: string;
  description?: string;
  notes?: string;
  artifact_type?: string;
};

type ResearchManifest = {
  datasets: ResearchEntry[];
};

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const repoRoot = resolve(__dirname, "..", "..");
const manifestPath = resolve(
  repoRoot,
  "data",
  "knowledge_base",
  "research",
  "manifest.json",
);
const readmePath = resolve(
  repoRoot,
  "data",
  "knowledge_base",
  "research",
  "README.md",
);

function toMarkdownTable(headers: string[], rows: string[][]): string {
  const headerRow = `| ${headers.join(" | ")} |`;
  const divider = `| ${headers.map(() => "---").join(" | ")} |`;
  const body = rows.map((row) => `| ${row.join(" | ")} |`).join("\n");
  return [headerRow, divider, body].filter(Boolean).join("\n");
}

function renderRegistry(entries: ResearchEntry[]): string {
  if (!entries.length) {
    return "(No entries recorded yet.)";
  }

  const rows = entries.map((entry) => [
    `\`${entry.slug}\``,
    entry.description ?? "—",
    entry.notes ?? "—",
  ]);

  return toMarkdownTable(["Slug", "Description", "Notes"], rows);
}

async function main() {
  const manifest: ResearchManifest = JSON.parse(
    await readFile(manifestPath, "utf-8"),
  );

  const entries = manifest.datasets ?? [];
  const datasetEntries = entries.filter((entry) =>
    !entry.artifact_type || entry.artifact_type === "dataset"
  );
  const modelEntries = entries.filter((entry) =>
    entry.artifact_type && entry.artifact_type !== "dataset"
  );

  const datasetTable = renderRegistry(datasetEntries);
  const modelTable = renderRegistry(modelEntries);

  const lines = [
    "<!-- DO NOT EDIT: generated by scripts/knowledge_base/sync-research-readme.ts -->",
    "# Research Knowledge Base Staging",
    "",
    "This folder documents the research-oriented corpora that were recently added to",
    "`OneDrive\\\\DynamicAI_DB\\\\knowledge_base\\\\research`. Use it as the control point",
    "for mirroring the new material into the Dynamic Capital knowledge base.",
    "",
    "## Source information",
    "",
    "- **OneDrive path:** `OneDrive\\\\DynamicAI_DB\\\\knowledge_base\\\\research`",
    "- **Supabase mirror (planned):**",
    "  `public.one_drive_assets/knowledge_base/research/*`",
    "- **Status:** Assets were uploaded to OneDrive and are awaiting checksum",
    "  verification before being copied into Supabase cold storage.",
    "",
    "## Mirroring checklist",
    "",
    "1. Authenticate against the DynamicAI OneDrive tenant and navigate to the",
    "   `knowledge_base/research` folder.",
    "2. Download the latest dataset bundle(s) and record their filenames plus SHA-256",
    "   checksums in `manifest.json` (create the file if it does not yet exist).",
    "3. Store the raw archives in the Supabase bucket path noted above and extract",
    "   text-forward assets into the RAG preprocessing pipeline.",
    "4. Update the registry below with a short description of each dataset so downstream",
    "   teams can request the appropriate slices for their experiments.",
    "",
    "## Dataset registry",
    "",
    datasetTable,
  ];

  if (modelEntries.length) {
    lines.push(
      "",
      "### Model registry",
      "",
      modelTable,
    );
  }

  lines.push(
    "",
    "### Extraction helpers",
    "",
    "Install the lightweight ML dependencies and materialise the Nemotron personas",
    "into JSONL form with:",
    "",
    "```bash",
    "pip install -r ml/requirements.txt",
    "python ml/extract_nemotron_personas_japan.py \\",
    "  --limit 3600 \\",
    "  --output data/knowledge_base/research/processed/nemotron_personas_japan.jsonl \\",
    "  --summary data/knowledge_base/research/processed/nemotron_personas_japan_summary.json",
    "```",
    "",
    "Adjust `--limit` (or omit it) depending on whether you need a quick sample or",
    "the full six-million persona corpus. The summary report captures metadata counts",
    "and deduplication statistics for downstream validation.",
    "",
    "## Latest training pass",
    "",
    "Execute the following commands after syncing the OneDrive drop to reproduce the",
    "current training artefacts:",
    "",
    "```bash",
    "python ml/preprocess_corpus.py \\",
    "  --input data/dhivehi_radheef_sample.jsonl data/dhivehi_radheef_pages_026_050.jsonl \\",
    "  --output data/knowledge_base/research/processed/dhivehi_training_corpus.jsonl \\",
    "  --languages en dv \\",
    "  --min-characters 64",
    "",
    "python ml/research_corpus_trainer.py \\",
    "  --dataset data/knowledge_base/research/processed/dhivehi_training_corpus.jsonl \\",
    "  --output data/knowledge_base/research/training_runs/dhivehi_radheef_v1.json \\",
    '  --objective "knowledge-base-research-dhivehi"',
    "```",
    "",
    "The resulting readiness summary recommends promoting the candidate checkpoint",
    "while tightening loss-focused regularisation before scaling further runs.",
    "",
    "Keep this document in sync with the upstream OneDrive folder so that future",
    "knowledge base drops reflect the new research materials.",
    "",
    "Run `npx tsx scripts/knowledge_base/sync-research-readme.ts` after updating",
    "`manifest.json` to regenerate this file.",
  );

  const content = `${lines.join("\n")}\n`;
  await writeFile(readmePath, content, "utf-8");
}

await main();
